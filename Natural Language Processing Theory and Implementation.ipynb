{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e8cc0",
   "metadata": {},
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Task In Natural Language Processing*</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c67e26",
   "metadata": {},
   "source": [
    "<h3 style=\"color:gray; line-height:1.2;\">1. Import the General libraries, NLP module, and Machine learning modules.<br>\n",
    "                        2. Load the dataset.<br>\n",
    "                        3. Text Preprocessing:<br>\n",
    "                        &ensp;&ensp;&ensp; i.   Removing html tags <br>\n",
    "                        &ensp;&ensp;&ensp; ii.  Removing Punctuations <br>\n",
    "                        &ensp;&ensp;&ensp; iii. Performing stemming <br>\n",
    "                        &ensp;&ensp;&ensp; iv.  Removing Stop words <br>\n",
    "                        &ensp;&ensp;&ensp; v.   Expanding contractions. <br>\n",
    "                        4. Apply Tokenization. <br>\n",
    "                        5. Apply Stemming. <br>\n",
    "                        6. Apply POS Tagging. <br>\n",
    "                        7. Apply Lemmatization. <br>\n",
    "                        8. Apply label encoding. <br>\n",
    "                        9. Feature Extraction. <br>\n",
    "                        10 Text to Numerical vector conversion:<br>\n",
    "                        &ensp;&ensp;&ensp; i.   Apply BOW(Count-Vectorizer). <br>\n",
    "                        &ensp;&ensp;&ensp; ii.  Apply TFIDF vectorizer. <br>\n",
    "                        &ensp;&ensp;&ensp; iii. Apply Word2Vector vectorizer. <br>\n",
    "                        &ensp;&ensp;&ensp; iv.  Apply Glove. <br>\n",
    "                        11. Data preprocessing. <br>\n",
    "                        12. Model Building. <br>\n",
    "                        13. Evaluate the model:<br>\n",
    "                        &ensp;&ensp;&ensp; i.   Confusion Matrix. <br>\n",
    "                        &ensp;&ensp;&ensp; ii.  Classification report. <br>\n",
    "                        14. Tracking experiments with the help of MLFlow. <br>\n",
    "                        15. Break code into production ready script. <br>\n",
    "                        16. Automation using Workflow Orchestration using Prefect.<br>\n",
    "                        17. Build Streamlit Data app to check whether pair of questions or similar or not.<br>\n",
    "                        18. Deploy Streamlit app to AWS Cloud.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf963dc",
   "metadata": {},
   "source": [
    "<h1 style=\"background:lightblue; color:blue; line-height:3; text-align:center; font-family:Arial Black\">* Terms Used in NLP *</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e57d05",
   "metadata": {},
   "source": [
    "<h3><div style=\" line-height:1.2\">\n",
    "    <b style=\"color:orange;\">Document &emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Each row in dataset is called Document.</b><br>\n",
    "    <b style=\"color:orange;\">Corpus &nbsp;&emsp;&emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Collection of Documents(all rows) is called Corpus.</b><br>\n",
    "    <b style=\"color:orange;\">Vocabulary &ensp;&emsp;:&ensp; </b><b style=\"color:gray\">Unique Words in Corpus</b><br>\n",
    "    <b style=\"color:orange;\">Segmentation &nbsp;:&ensp; </b><b style=\"color:gray\">Breaking multiple sentences into single individual sentence is called Segmentation.</b><br>\n",
    "    <b style=\"color:orange;\">Tokenization &nbsp;&ensp;:&ensp; </b><b style=\"color:gray\">Process of breaking sentence into Words is called Tokenization and the words are called Tokens.</b><br> \n",
    "    <b style=\"color:orange;\">StopWords &ensp;&emsp;:&ensp; </b><b style=\"color:gray\">Common words used in any language are called Stop-Words</b><br>\n",
    "    <b style=\"color:orange;\">Stemming &emsp;&emsp;:&ensp; </b><b style=\"color:gray\">Process of removing or replacing suffixes of word to get the root or base word is called <br> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Stemming. But sometimes meaning of word will lost.</b><br>\n",
    "    <b style=\"color:orange;\">Lemmatization :&ensp; </b><b style=\"color:gray\">Process of removing or replacing suffixes of word to get the root or base word is called <br> &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Lemmatization. Here words have dictionary meaning.</b><br>\n",
    "    <b style=\"color:orange;\">NER Tagging &ensp;&nbsp;:&ensp; </b><b style=\"color:gray\">Process of Adding Tags to each word like \"Person, Place, Currency\" etc. is called NER Tagging.</b><br>\n",
    "     <b style=\"color:orange;\">POS Tagging &ensp;&nbsp;:&ensp; </b><b style=\"color:gray\">Process of Adding Part of Speech Tags to each word is called POS Tagging..</b><br>\n",
    "    <b style=\"color:orange;\">Chunking &emsp;&nbsp;&emsp;:&ensp; </b><b style=\"color:gray\">Process of Conversion of sentence to a flat tree is called Chunking.</b><br>\n",
    "</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23cb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914182ba",
   "metadata": {},
   "source": [
    "<h1 style=\"background:pink; color:blue; line-height:2.2; text-align:center; font-family:Arial Black\">Import Required Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c615b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e76ab",
   "metadata": {},
   "source": [
    "<h1 style=\"background:pink; color:blue; line-height:2.2; text-align:center; font-family:Arial Black\">Load the Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c71a59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af034c6",
   "metadata": {},
   "source": [
    "<h1 style=\"background:pink; color:blue; line-height:2.2; text-align:center; font-family:Arial Black\">Understanding the Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8867a6",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; text-align:left; font-family:Arial Black\">1. Shape of Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f0629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset:(404290, 6)\n",
      "Number of Columns:6\n",
      "Number of Rows:404290\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of dataset:{}\\nNumber of Columns:{}\\nNumber of Rows:{}\".format(df.shape,df.shape[1],df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7a7e2",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; text-align:left; font-family:Arial Black\">2. Columns in Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5202931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Columns:6\n",
      "Columns are:Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Columns:{}\\nColumns are:{}\".format(len(df.columns), df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1768a",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; text-align:left; font-family:Arial Black\">3. Data Types of Column</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfbea04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "qid1             int64\n",
       "qid2             int64\n",
       "question1       object\n",
       "question2       object\n",
       "is_duplicate     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc12fd3",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; text-align:left; font-family:Arial Black\">4. Column Information</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb311c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   id            404290 non-null  int64 \n",
      " 1   qid1          404290 non-null  int64 \n",
      " 2   qid2          404290 non-null  int64 \n",
      " 3   question1     404289 non-null  object\n",
      " 4   question2     404288 non-null  object\n",
      " 5   is_duplicate  404290 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0aaf93",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; text-align:left; font-family:Arial Black\">5. Missing Values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71cccf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       1\n",
       "question2       2\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d4299",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">There is 1 Missing value in `Question1` and 2 Missing values in `Question2`. Lets check these row and decide whether to drop these rows or fill with some suitable value.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81888e8a",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">6. Missing value Treatment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b898709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105780</th>\n",
       "      <td>105780</td>\n",
       "      <td>174363</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201841</th>\n",
       "      <td>201841</td>\n",
       "      <td>303951</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                         question1 question2  \\\n",
       "105780  105780  174363  174364    How can I develop android app?       NaN   \n",
       "201841  201841  303951  174364  How can I create an Android app?       NaN   \n",
       "\n",
       "        is_duplicate  \n",
       "105780             0  \n",
       "201841             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['question2'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbf09e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363362</th>\n",
       "      <td>363362</td>\n",
       "      <td>493340</td>\n",
       "      <td>493341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My Chinese name is Haichao Yu. What English na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2 question1  \\\n",
       "363362  363362  493340  493341       NaN   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "363362  My Chinese name is Haichao Yu. What English na...             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['question1'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470e9a8",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">We Cannot compute these missing rows with any suitable value, hence it is better to drop these rows.</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1981f5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "qid1            0\n",
       "qid2            0\n",
       "question1       0\n",
       "question2       0\n",
       "is_duplicate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3ba6c",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Now we don't have any Missing values in Dataset</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f460fd6",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">7. Duplicate Rows</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830eacd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48770bb9",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">No Duplicate rows in Dataset</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc83bec",
   "metadata": {},
   "source": [
    "<h1 style=\"background:pink; color:blue; text-align:center; line-height:2.2; font-family:Arial Black\">Text Pre-Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c87b7",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Text preprocessing is a crucial step in NLP. Cleaning our text data in order to convert it into a presentable form that is analyzable and predictable for our task is known as text preprocessing.<br>\n",
    "Many steps can be taken in text preprocessing, few steps are,<br>\n",
    "<b style=\"color:orange\">A. Basic Techniques:</b><br>\n",
    "    &ensp;&ensp;1. Lowering Case<br>\n",
    "    &ensp;&ensp;2. Remove Punctuations<br>\n",
    "    &ensp;&ensp;3. Removal of special characters and Numbers<br>\n",
    "    &ensp;&ensp;4. Removal of HTML tags<br>\n",
    "    &ensp;&ensp;5. Removal of URL's<br>\n",
    "    &ensp;&ensp;6. Removal of Extra Spaces<br>\n",
    "    &ensp;&ensp;7. Expanding Contraction<br>\n",
    "    &ensp;&ensp;8. Text Correction<br>\n",
    "<b style=\"color:orange\">B. Advanced Techniques:</b><br>\n",
    "    &ensp;&ensp;1. Apply Tokenization<br>\n",
    "    &ensp;&ensp;2. Stop Word Removal<br>\n",
    "    &ensp;&ensp;3. Apply Stemming<br>\n",
    "    &ensp;&ensp;4. Apply Lemmatization<br>\n",
    "<b style=\"color:orange\">C. More Advanced Techniques:</b><br>\n",
    "    &ensp;&ensp;1. POS(Part Of Speech) Tagging<br>\n",
    "    &ensp;&ensp;2. NER(Name Entity Recognation)<br></p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72559",
   "metadata": {},
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">A. Basic Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506c4c1",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. Lowering Case</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be337ed1",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\"><b style=\"color:black\"> Lowering Case of text is essential step in text preprocessing due to following reasons:</b><br>\n",
    "    &emsp;&emsp; 1. The same words, one in upper case and other in lower case are considered as different words while creating\n",
    "                    BOW, hence lowering add the same value for both the words.<br>\n",
    "    &emsp;&emsp; 2. In TF-IDF CountVectorization techniques the frequency of words is considered with irrespective of the case.<br>\n",
    "    &emsp;&emsp; 3. Lowering decreasing the size of the vocabulary and hence reduce the dimensionality.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c3f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: What is the STEP by step guide to invest In share market in india?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Lowered Sentence: what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"What is the STEP by step guide to invest In share market in india?\"\n",
    "sentence_lower=str(sentence).lower()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Lowered Sentence:\", sentence_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052b48e",
   "metadata": {},
   "source": [
    "**<code style=\"color:green;\">In the `Original Sentence` we have two `Step` with different cases and same meaning in sentence, after coverting everything to lower both words look similar and we reduced the dimensionality.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2420d7",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. Removing Punctuations</h3>\n",
    "<h3><p style=\"color:orange; line-height:1.1\">To remove Punctuations we are going to use python \"String\" library.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "026e546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582b504",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Above are the Punctuations any any language</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e9f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Question SImilirity ^ . We are actually happy !! Because we wanted this project * *\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without Punctuations: Hello Everyone, this is team Data Dynamos We are got an project of Quora Question SImilirity We are actually happy !! Because we wanted this project\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Question SImilirity ^ . We are actually happy !! Because we wanted this project * *\"\n",
    "without_punc=[word for word in sentence.split(\" \") if word not in list(punc)]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without Punctuations:\", \" \".join(without_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a86756",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">3. Removing Special Characters and Numbers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace12224",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Special Characters and numbers like \"!,@,#,%,^,&,$,+,*, 1 to 9\" have no meaning in the sentence and they do not contribute to any sentence classification. And there is one senario when these special characters attached to any word will considered as different word which is already present in the sentence. eg. \"Shocked\" and \"Shocked!\" considered as different words but we know they have same meaning. Hence its better to remove any special characters there for dimensionality is also reduces.<br>\n",
    "<b style=\"color:orange\">We are going to use python \"re package\" to remove special characters and numbers.</b></p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34015f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Find the remainder when  math          math  is divided by       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence=\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
    "sentence_clean=re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168f0f6",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">In `Original Sentence` \"{},[],/,?,^\" are the special characters, `Clean Sentence` contains no special characters and numbers.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d44a7f",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">4. Removal of HTML Tags</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e157b1",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">When we Scrap data from any website then dataset contains HTML tags. We might face problem if HTML Tags present in our dataset. Hence it prefered to remove these tags.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f854cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: <h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hello Guys How Are You\n"
     ]
    }
   ],
   "source": [
    "sentence='''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
    "clean_sentence=re.sub(\"<.*?>\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5d40a",
   "metadata": {},
   "source": [
    "**<code style=\"color:green;\">The Original Sentence contains HTML tags, after removing these tags using re.sub function of python regex, our Sentence looks human readable.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc8f23",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">5. Removing URL's</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddcdab",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">Some times in the Quora question people provide some external links and url's. As we know that the urls are the random combinations of strings which does not cotains any specific meaning. Hence is useful to remove thes urls.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8a7f5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis link and I found very interesting sentiment analysis projects.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: I visited  link and I found very interesting sentiment analysis projects.\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis link and I found very interesting sentiment analysis projects.\"\n",
    "clean_sentence=re.sub(r\"(http|https|www)\\S+\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e60bd",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Original sentence conatins an external website link, which cause problem in our analysis. So after removing this link check the clean sentence, with no url.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb545d96",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">6. Removing Extra Spaces</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fc2eb",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1\">There is some senario where users insert extra spaces at the start, at the end or at the anywhere in the sentence. We need to remove all the extra spaces inserted by an user.</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92acf6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hi Team    Data Dynamos, How is your     project going on            ?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hi Team Data Dynamos, How is your project going on ?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hi Team    Data Dynamos, How is your     project going on            ?\"\n",
    "clean_sentence=re.sub(\" +\",\" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19abd37",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">7. Expanding Contraction</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd94f4d",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1.2\">Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe. Nowadays, where everything is shifting online, we communicate with others more through text messages or posts on different social media like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, etc. in the form of texts. With so many people to talk, we rely on abbreviations and shortened form of words for texting people.<br><br>\n",
    "<b style=\"color:orange\">We need to exapnd these contractions so that we can easliy apply tokenization and normalization(stemming and lemmatization). Here we are going to use <b style=\"color:green\">contrations</b> python library to exapand the constraction words.</b></p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4505037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f224ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We've reached final step of our data science internship. We'll meet u in project presentation.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clear Sentence: We have reached final step of our data science internship. We will meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "sentence=\"We've reached final step of our data science internship. We'll meet u in project presentation.\"\n",
    "clear_sentence=contractions.fix(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clear Sentence:\", clear_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd019559",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Original Sentence contains contraction words like `\"We've\",\"We'll\",\"u\"`. And the expanded words for these constraction are `\"We have\",\"We will\", \"You\"`.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b4c18",
   "metadata": {},
   "source": [
    "<p> <h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">8. Text Correction</h3>\n",
    "<h3 style=\"color:orange\">To correct the text we are going to use TextBlob from NLTK</h3></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07535483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Correct Sentence: He have reached final step of our data science Training. He'll meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence=\"We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\"\n",
    "textblob=TextBlob(sentence)\n",
    "correct_sentence=textblob.correct()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Correct Sentence:\", correct_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448cd94",
   "metadata": {},
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">Level-2 Techniques or Advanced Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc88b78",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. Apply Tokenization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81290a01",
   "metadata": {},
   "source": [
    "<h3><p style=\"color:gray; line-height:1.1\">Tokenization is a process of breaking down sentence into words. These words are called Tokens. Here, tokens can be either words, characters, or subwords.<br>\n",
    "<b style=\"color:orange\">Tokenization is broadly classified into 3 types:</b><br>\n",
    "    &ensp;&ensp;a. Sentence Tokenization<br>\n",
    "    &ensp;&ensp;b. Word Tokenization<br>\n",
    "    &ensp;&ensp;c. SubWord(n-gram characters) Tokenization<br><br>\n",
    "<b style=\"color:orange\">Here we can use string \"Split\" method for word tokenization only. For Charcter and SubWord Tokenization we need to use \"NLTK\" inbuit funvtion.</b></p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f28d38",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Sentence Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "830380d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence Tokens: ['Our Team name is Team Data Dynamos and we have selected Quora question similarity project.', 'We have started working on this project from 13th of May only.', 'Working with team gives little extra space to apply new things.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.'''\n",
    "tokens=sent_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed1fec",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. Word Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "332cf71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project.?']\n"
     ]
    }
   ],
   "source": [
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?'''\n",
    "tokens=sentence.split(\" \")\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "472485fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?'''\n",
    "tokens=word_tokenize(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892c2b6",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">We can easily see the difference, when we tokenize using string method, it will consider all the special characters & punctuation attached to a word as a part of that word, but when we tokenize using NLTK word_tokenizer it consider those special characters & punctuation as a seperate toke.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca250a",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">c. Sub-Word(n-gram character) Tokenization</h3>\n",
    "<h3 style=\"color:gray; line-height:1.1\">N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document.<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae208ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26eec001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "N-gram Tokens: [('Our', 'Team', 'name'), ('Team', 'name', 'is'), ('name', 'is', 'Team'), ('is', 'Team', 'Data'), ('Team', 'Data', 'Dynamos'), ('Data', 'Dynamos', 'and'), ('Dynamos', 'and', 'we'), ('and', 'we', 'have'), ('we', 'have', 'selected'), ('have', 'selected', 'Quora'), ('selected', 'Quora', 'question'), ('Quora', 'question', 'similarity'), ('question', 'similarity', 'project.'), ('similarity', 'project.', 'We'), ('project.', 'We', 'have'), ('We', 'have', 'started'), ('have', 'started', 'working'), ('started', 'working', 'on'), ('working', 'on', 'this'), ('on', 'this', 'project'), ('this', 'project', 'from'), ('project', 'from', '13th'), ('from', '13th', 'of'), ('13th', 'of', 'May'), ('of', 'May', 'only.'), ('May', 'only.', 'Working'), ('only.', 'Working', 'with'), ('Working', 'with', 'team'), ('with', 'team', 'gives'), ('team', 'gives', 'little'), ('gives', 'little', 'extra'), ('little', 'extra', 'space'), ('extra', 'space', 'to'), ('space', 'to', 'apply'), ('to', 'apply', 'new'), ('apply', 'new', 'things.')]\n"
     ]
    }
   ],
   "source": [
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.'''\n",
    "n_gram_tokens=list(ngrams((sentence.split(\" \")), n=3))\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"N-gram Tokens:\", n_gram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1471d29",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. Remove Stop Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc249474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stop Words in English= 179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en=stopwords.words(\"english\")\n",
    "print(\"Total Stop Words in English=\", len(stopwords_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8541c9c",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">English language contains 179 Stop WOrds.</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458c76a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with StopWOrds: Our Team name is Team Data Dynamos and we have selected Quora question similarity project\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without StopWOrds: Our Team name Team Data Dynamos selected Quora question similarity project\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Our Team name is Team Data Dynamos and we have selected Quora question similarity project\"\n",
    "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_en]\n",
    "print(\"Sentence with StopWOrds:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without StopWOrds:\", \" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700af43",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">3. Apply Stemming</h3>\n",
    "<h3><p style=\"color:orange;\"><b>Types of Stemmer in NLP:</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. Prter Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. SnowBall Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">c. Lancaster Stemmer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">d. Regexp Stemmer</b><br>\n",
    "</p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a9ccb",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Porter Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Porter Stemmer is the original stemmer but the stem sometimes illogical or non-dictionary word.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90e5ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "porter_stem=[porter.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(porter_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d703d0c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. Snowball Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Snowball stemmer is faster and more logical than the Porter Stemmer.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3243fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball=SnowballStemmer(language=\"english\")\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "snowball_stem=[snowball.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(snowball_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc40e",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">c. Lancaster Stemmer</h3>\n",
    "<h3 style=\"color:gray\">The Lancaster stemmers are more aggressive and dynamic. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. Lancaster Stemmer produces results with excessive stemming.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de4ba3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect driv driv driv abl abl en en en\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "lancaster_stem=[lancaster.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873ad43",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">d. Regexp Stemmer</h3>\n",
    "<h3 style=\"color:gray\">Regexp stemmer identifies morphological affixes using regular expressions. Substrings matching the regular expressions will be discarded.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae7b72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Porter Stemming: Connect Connection Connection Connect Connected Connect Connecting Driv Driven Drive Abl Able Enabl Enable Enabl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regex=RegexpStemmer(regexp=\"ing$|s$|e$\", min=0)\n",
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "regex_stem=[regex.stem(word) for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Porter Stemming:\", \" \".join(regex_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cf5c6",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">All Stemmers are Different from each other. Ther is one common thing between all stemmers, sometimes they did not return the stem with logical or dictionary meaning.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf45fb",
   "metadata": {},
   "source": [
    "<h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">4. Apply Lemmatization</h3>\n",
    "<h3><p style=\"color:orange;\"><b>Types of Lemmatization in NLP:</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. Wordnet Lemmatizer</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. TextBlob Lemmatizer</b><br>\n",
    "</p></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c25f7",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. Wordnet Lemmatizer</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4886625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I woke up today too early and I came outside to see rising sun\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: I wake up today too early and I come outside to see rise sun\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma=WordNetLemmatizer()\n",
    "sentence=\"I woke up today too early and I came outside to see rising sun\"\n",
    "sentence_lemma=[lemma.lemmatize(word, 'v') for word in sentence.split(\" \")]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94858bc8",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. TextBlob Lemmatizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8725ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The bats are hanging on their feet in upright positions\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence after Lemmatization: The bat are hanging on their foot in upright position\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "sentence=\"The bats are hanging on their feet in upright positions\"\n",
    "sent=TextBlob(sentence)\n",
    "texblob_lemma=[w.lemmatize() for w in sent.words]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence after Lemmatization:\", \" \".join(texblob_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa87baf",
   "metadata": {},
   "source": [
    "<h2 style=\"background:pink; color:blue; line-height:1.7; font-family:Arial Black; text-align:left\">Level-3 Techniques or More Advanced Techniques</h2>\n",
    "<p><h3 style=\"color:gray;\">These Techniques are not used in all the tasks, these are problem specific. These techniques are mainly used in QA System(Question Answer), Word Sense Disambiguiation etc.</h3>\n",
    "<h3 style=\"color:orange;\">More Advanced Techniques are:<br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">1. POS(Part of Speech) Tagging</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">2. NER(Name Entity Recognisation) Tagging</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fd7fd",
   "metadata": {},
   "source": [
    "<p><h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">1. POS Tagging</h3>\n",
    "<h3 style=\"color:gray;\">Adding a Part of Speech tags to every word in the corpus is called POS tagging. If we want to perform POS tagging then no need to remove stopwords. This is one of the essential steps in the text analysis where we know the sentence structure and which word is connected to the other, which word is rooted from which, eventually, to figure out hidden connections between words which can later boost the performance of our Machine Learning Model.<br>\n",
    "    <b style=\"color:orange\">POS Tagging can be performed using two Libraries</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. POS Tagging using NLTK</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. POS Tagging using Spacy</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c7cb2",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. POS Tagging using NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f61bfc5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS Tag: WP\n",
      "Word: is || POS Tag: VBZ\n",
      "Word: the || POS Tag: DT\n",
      "Word: step || POS Tag: NN\n",
      "Word: by || POS Tag: IN\n",
      "Word: step || POS Tag: NN\n",
      "Word: guide || POS Tag: RB\n",
      "Word: to || POS Tag: TO\n",
      "Word: invest || POS Tag: VB\n",
      "Word: in || POS Tag: IN\n",
      "Word: share || POS Tag: NN\n",
      "Word: market || POS Tag: NN\n",
      "Word: in || POS Tag: IN\n",
      "Word: india || POS Tag: NN\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "doc=word_tokenize(\"What is the step by step guide to invest in share market in india\")\n",
    "for i in range(len(doc)):\n",
    "    print(\"Word:\",pos_tag(doc)[i][0], \"||\", \"POS Tag:\", pos_tag(doc)[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb7ccf",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. POS Tagging using Spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c546395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af661e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS: PRON || POS Tag: WP || Explanation: wh-pronoun, personal\n",
      "Word: is || POS: AUX || POS Tag: VBZ || Explanation: verb, 3rd person singular present\n",
      "Word: the || POS: DET || POS Tag: DT || Explanation: determiner\n",
      "Word: step || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: by || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: step || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: guide || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: to || POS: PART || POS Tag: TO || Explanation: infinitival \"to\"\n",
      "Word: invest || POS: VERB || POS Tag: VB || Explanation: verb, base form\n",
      "Word: in || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: share || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: market || POS: NOUN || POS Tag: NN || Explanation: noun, singular or mass\n",
      "Word: in || POS: ADP || POS Tag: IN || Explanation: conjunction, subordinating or preposition\n",
      "Word: india || POS: PROPN || POS Tag: NNP || Explanation: noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"What is the step by step guide to invest in share market in india\")\n",
    "for word in doc:\n",
    "    print(\"Word:\", word.text,\"||\",\"POS:\", word.pos_, \"||\", \"POS Tag:\", word.tag_, \"||\", \"Explanation:\", spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210abf8",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Spacy is more powerful than NLTK. Spacy is faster and Grammatically accurate.</code>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d11bec",
   "metadata": {},
   "source": [
    "<p><h3 style=\"background:pink; color:blue; line-height:1.5; font-family:Arial Black; text-align:left\">2. NER Tagging</h3>\n",
    "<h3 style=\"color:gray;\">Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as well as themes, topics, times, monetary values and percentages.<br>\n",
    "    <b style=\"color:orange\">NER can be performed using two Libraries</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">a. NER using NLTK</b><br>\n",
    "    &emsp;&emsp;&emsp;<b style=\"color:gray\">b. NER using Spacy</b><br>\n",
    "</h3></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6e9d7",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">a. NER using NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "205d2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords_en=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e76655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORGANIZATION TATA/NNP Mahindra/NNP)\n",
      "('top', 'JJ')\n",
      "('companies', 'NNS')\n",
      "('India.', 'NNP')\n",
      "('But', 'CC')\n",
      "(\"'Gautam\", 'NNP')\n",
      "(\"Adani'\", 'NNP')\n",
      "(\"'Mukesh\", 'POS')\n",
      "(\"Ambani'\", 'NNP')\n",
      "('reachest', 'NN')\n",
      "('person.', 'NN')\n"
     ]
    }
   ],
   "source": [
    "sentence=\"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani' are the reachest person.\"\n",
    "words=[word for word in sentence.split(\" \") if word not in stopwords_en]\n",
    "tagged_tokens=nltk.pos_tag(words)\n",
    "entities=nltk.ne_chunk(tagged_tokens)\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872959c",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue; background:pink; line-height:1\">b. NER using Spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18825ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATA ORG\n",
      "Mahindra ORG\n",
      "India GPE\n",
      "Gautam Adani' PERSON\n",
      "Mukesh Ambani' PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence=\"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani' are the reachest person.\"\n",
    "doc = nlp(sentence)\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fbb85",
   "metadata": {},
   "source": [
    "**<code style=\"color:green\">Spacy is a faster and more efficient library for NER. It provides a pre-trained NER model that is highly accurate than NLTK and can recognize a wide range of named entities. Additionally, SpaCy has more advanced features such as named entity linking and coreference resolution.</code>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2615b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
